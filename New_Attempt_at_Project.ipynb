{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brittanyasare04/milestone3/blob/main/New_Attempt_at_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVb9MOrO5tC-"
      },
      "source": [
        "# DS 3001 Project\n",
        "*by Brittany Asare, Hewan Kasie, and Simone Minor*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmTx05t65Zhh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZYxwmCi50Ko"
      },
      "source": [
        "Abstract: A one paragraph summary of the question, methods, and results (about\n",
        "350 words)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vch0dAi504Q"
      },
      "source": [
        "The current job market in the United States has undergone significant changes in recent decades, creating unique challenges for individuals seeking employment, particularly for recent college graduates suffering significantly. Our project seeks to investigate the labor market outcomes with a focus on demographic factors such as race, gender, and field of study shape opportunities and barriers in securing employment. Utilizing data from IPUMS, drawn from national surveys including the NSCG, SDR, NSRCG, and ISDR, we constructed a dataset that encompasses the decades of labor force experiences. This dataset allows us to analyze a wide range of variables, including technical skill, highest degree earned, employment status, occupation, employer characteristics, income, job characteristics, job satisfaction, and career trajectory. After cleaning and preparing our data, we applied statistical and visualization methods to identify disparities in employment access and outcomes. Our finding suggests… [TBD]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzfQlBQ856Wa"
      },
      "source": [
        "Data: One to two pages discussing the data and key variables, and any challenges in reading, cleaning, and preparing them for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn2m2DZ957I8"
      },
      "source": [
        "- Technical Variables\n",
        "- Demographic Variables\n",
        "- Bachelor's Degree Variables\n",
        "- Highest Degree Variables\n",
        "- Employment Variables\n",
        "- Occupation Variables\n",
        "- Employer Characteristics Variables\n",
        "- Income Variables\n",
        "- Job Characteristics Variables\n",
        "- Job Satisfaction Variables\n",
        "- Career Path Jobs Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNw1zVqX6Bxl"
      },
      "source": [
        "PERSONID (Individual identification number)\n",
        "YEAR (Survey year)\n",
        "WEIGHT (SESTAT weight)\n",
        "SAMPLE (Sample identifier)\n",
        "SURID (Survey identifier)\n",
        "AGE (Age)\n",
        "GENDER (Gender)\n",
        "MINRTY (Minority indicator)\n",
        "RACETH (Race/ethnicity)\n",
        "CHTOT (Total number of children)\n",
        "NBAMED (Field of major for first bachelor degree code)\n",
        "DGRDG (Type of highest certificate or degree)\n",
        "NDGMED (Field of major for highest degree )\n",
        "NDGMEMG (Field of major for highest degree (major group))\n",
        "HDRGN (Location of school awarding highest degree)\n",
        "HDFLN (Financial support for highest degree: loans)\n",
        "SAMEFLD (Likelihood of choosing same field of study for highest degree)\n",
        "LFSTAT (Labor force status)\n",
        "HRSWK (Principal job hours worked)\n",
        "OCEDRLP (Principal job related to highest degree)\n",
        "NOCPR (Job code for principal job)\n",
        "EMSEC (Employer sector)\n",
        "GOVSUP (Federal government support indicator)\n",
        "SALARY (Salary (annualized))\n",
        "NRREA (Most important reason for working outside field of highest degree)\n",
        "NRSEC (Second most important reason for working outside field of highest degree)\n",
        "JOBSATIS (Job satisfaction)\n",
        "NWLAY (Reasons for not working: layoff)\n",
        "NWNOND (Reasons for not working: did not need/want to work)\n",
        "NWOCNA (Reasons for not working: suitable job not available)\n",
        "NWSTU (Reasons for not working: student)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQg9s9Ka6Fcb"
      },
      "source": [
        "Based on our interest in the current economic state of our nation, we decided to take a dive into the current job market and the difficulties that it portrays for those who are in search of employment. We extracted our data from IPUMS and got our data from a variety of sources, including surveys from across the last few decades, such as NSCG, SDR, NSRCG, and ISDR. In doing so, we were able to get a grasp on the current state of job market. This was a single dataset that we would later be able to derive an analysis from in the ease of securing a job in today’s economy.  \n",
        "\n",
        "The main variables that we analyzed from our dataset were the following: technical skill, demographic, highest degree achieved, employment, occupation, employer characteristics, income, job characteristics, job satisfaction, and career path jobs. These variables are essential to our research question-- what are the labor market outcomes for college graduates by major, race, and gender--  because they will better explain the reasoning as to how jobs are found, the capacity in which the job is described, and the feasibility of its accessibility to those who are in search of employment. We are also then able to cross-reference those variables with gender, race, and major of the recent graduate to analyze at which point the labor market is favorable towards certain demographic groups and what is expected from domains. The job market has become increasingly difficult to navigate, especially for new college graduates, as its requirements have become more specific. As a result, many job seekers struggle to find matching criteria that meet their needs. By identifying these variables, we are able to analyze their effects on the labor market selection and requirements, and their search results for those who are using it to find employment.\n",
        "\n",
        "In processing this data, the issues that we ran into consisted of recoding the missing data per variable. As we know, for many datasets, there are instances where the category goes unanswered per entry or there was no information provided. In order to address these concerns, we first had to identify the areas of the variable that resulted in either NaN or missing values. We would then go on to recode this area in each variable to ensure that it was no longer a string and rather a numeric value that would then be able to be used as a point in our analysis while not skewing our data.  \n",
        "\n",
        "Some limitations that may affect our analysis of the data are logical skipping points that were consistent through the data from the questions that respondents bypassed based on their previous responses. The “Logical Skips” are normally coded by a number, and it would be inaccurate and a misrepresentation to drop this data, but we are unsure of how it will affect the relationships we are measuring in our data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYprz5pqvIaG"
      },
      "source": [
        "*Brittany wrote the response*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEmFJmd-_Aee"
      },
      "source": [
        "Methods and Results: Two to four pages providing visualizations, statistics, tables, a discussion of your methodology, and a presentation of your main results. In\n",
        "particular, how are you approaching the problem? How confident are you about your\n",
        "assessments?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqKSxFE4_mQY"
      },
      "source": [
        "In order to examine the disparities and patterns of the labor market, we first need to clean the data. While cleaning the data, we referred to the user extract guide to ensure that we didn’t lose meaningful data in the process. At first, when we cleaned the data for the first submission of this project, but in dropping all missing and nonapplicable data, we accidentally dropped all the years with missing data which told an incomplete story about the job market. On our second revision we went through and simply recoded the columns to reflect cleaner data while also leaving the door open to have the data in its original form with each number representing a response and a “recoded” version where every response is reflected as words and the variables are ran as categorical. Doing this opened up the possibilities for what we were able to do by generating graphs and charts and ultimately drawing conclusions.  \n",
        "\n",
        "One chart type we are using is stacked bar graphs. Stacked bar graphs are great visuals for categorical variables to show how one categorical variable is distributed across another. We plan to make a few different iterations of these graphs and pull out the most meaningful ones in our final submission; however, we started to explore this visualization by looking at each category spread by year. This created numerous charts where each bar represents a year, and the colors correspond to the different responses. This visual clearly shows the spread within the variable for each year. Later, we want to flesh these out and focus on pairs of categorical variables beyond the year to discuss some trends within categories. Stacked bar charts maintain the total of the category while also dividing it into levels to tell an interesting story about the relationship between the two variables.\n",
        "\n",
        "Sankey diagrams are great at visualizing the flow and direct connections between variables. They are particularly great at taking the total of the population that responded and break it down by the levels of other categories. We want to visualize the flow of who is employed by different life, identity, and demographic factors, like race, gender, major, children, and more. We anticipate that we will also be able to use this graph to track change in employment over time, which may be contextualized with social, cultural, and political information of the time.  \n",
        "\n",
        "In addition, we have not yet experimented with it but plan to use mosaic plots to better visualize the proportions of relationships that contribute to employment and unemployment. This visualization will put all of the categories and their levels in relation to the grand scheme of the total of employed or unemployed. Lastly, we plan to use heat maps to show the relationship between categories. The colored grid will indicate the various levels, and the weights will determine the shade which represents the strength of the relationship between the two intersecting categories. Through these methods we hope to yield results that will reveal or deny a relationship between various academic and personal factors with one’s employment status to get a better sense of the job market and who it is best for.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pINYsDdsNLY"
      },
      "source": [
        "*Brittany did the inital visualization*\n",
        "\n",
        "*Hewan did the recoding\n",
        "Citiation: https://stackoverflow.com/questions/54052471/mapping-values-in-place-for-example-with-gender-from-string-to-int-in-pandas-d*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "zb6Ja9bC5ub3",
        "outputId": "5df61164-487e-46f3-e64a-9a03e6e10d55"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/highered_00001.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2521957387.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjobedu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/highered_00001.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/highered_00001.csv'"
          ]
        }
      ],
      "source": [
        "jobedu = pd.read_csv('/content/highered_00001.csv')\n",
        "# If the file is in a different location, please update the path accordingly, for example:\n",
        "# jobedu = pd.read_csv('/path/to/your/highered_00001.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "WorBzyRl7sFD",
        "outputId": "ce16f544-4010-4a23-fa35-454dea7dc7c2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'jobedu' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3122721778.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjobedu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'jobedu' is not defined"
          ]
        }
      ],
      "source": [
        "jobedu.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opQlCoyA8F58"
      },
      "outputs": [],
      "source": [
        "jobedu.info()\n",
        "\n",
        "categorical_columns = [\n",
        "    'YEAR', 'SAMPLE', 'SURID', 'GENDER', 'MINRTY', 'RACETH', 'CHTOT', 'DGRDG',\n",
        "    'NDGMEMG_clean', 'HDRGN_clean', 'HDFLN_clean', 'SAMEFLD', 'LFSTAT',\n",
        "    'OCEDRLP', 'EMSEC', 'GOVSUP', 'NRREA', 'NRSEC_clean', 'JOBSATIS', 'NWLAY',\n",
        "    'NWNOND', 'NWOCNA', 'NWSTU'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v87almmu8MAX"
      },
      "outputs": [],
      "source": [
        "#Initial visualization of the data to understand how to clean it\n",
        "\n",
        "for col in categorical_columns:\n",
        "    if col != 'YEAR': # Avoid grouping by Year with itself\n",
        "        try:\n",
        "            grouped_data = jobedu.groupby(['YEAR', col]).size().unstack()\n",
        "            if not grouped_data.empty:\n",
        "                grouped_data.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "                plt.title(f'Distribution of {col} by Year')\n",
        "                plt.xlabel('Year')\n",
        "                plt.ylabel('Count')\n",
        "                plt.legend(title=col, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"Could not create plot for {col}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "507b05e2"
      },
      "outputs": [],
      "source": [
        "# Creating a new dataset by copying the original DataFrame\n",
        "jobedu_recoded = jobedu.copy()\n",
        "\n",
        "#TEST to see if the mapping works\n",
        "# Define a mapping for the 'GENDER' column\n",
        "gender_mapping = {1: 'male', 2: 'female'}\n",
        "\n",
        "jobedu_recoded['GENDER_recoded'] = jobedu_recoded['GENDER'].map(gender_mapping)\n",
        "\n",
        "# Displaying the first few rows of the new dataset to see the new column\n",
        "display(jobedu_recoded.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIZf8rWLD17_"
      },
      "outputs": [],
      "source": [
        "# Define the mappings for the additional columns\n",
        "minrty_mapping = {1: 'minority', 0: 'not minority'}\n",
        "raceth_mapping = {1: 'Asian', 2: 'White', 3: 'Under-represented Minority', 4: 'Other'}\n",
        "CHTOT_mapping = {0: 'No Children', 1: 'One Child', 2: 'One to Three Children', 3: 'Two or More Children', 4: 'More Than Three Children', 98: 'Not Applicable'}\n",
        "nbamed_mapping = {\n",
        "    198895: 'Computer and mathematical science',\n",
        "    226395: 'Biological sciences',\n",
        "    298895: 'Other biological, agricultural, environmental life sciences',\n",
        "    318730: 'Chemistry, except biochemistry',\n",
        "    338785: 'Physics and astronomy',\n",
        "    388795: 'Physical and related sciences',\n",
        "    398895: 'Other physical and related sciences',\n",
        "    419295: 'Economics',\n",
        "    429295: 'Political and related sciences',\n",
        "    438995: 'Psychology',\n",
        "    449995: 'Sociology and anthropology',\n",
        "    459395: 'Other social sciences',\n",
        "    489395: 'Social and related sciences',\n",
        "    527250: 'Chemical engineering',\n",
        "    537260: 'Civil engineering',\n",
        "    547280: 'Electrical, electronics and communications engineering',\n",
        "    567350: 'Mechanical engineering',\n",
        "587995: 'Other engineering',\n",
        "    611995: 'Health-related fields',\n",
        "    699995: 'Other science and engineering-related',\n",
        "    719995: 'Management and administration',\n",
        "    799995: 'Other non-science and engineering',\n",
        "    999996: 'Blank',\n",
        "    999998: 'Logical skip',\n",
        "    999999: 'Missing'\n",
        "}\n",
        "\n",
        "dgrdg_mapping = {1: 'Bachelors', 2: 'Masters', 3: 'Doctorate', 4: 'Professional'}\n",
        "ndgmed_mapping = {\n",
        "    198895: 'Computer and mathematical sciences',\n",
        "    226395: 'Biological sciences',\n",
        "    298895: 'Other biological, agricultural, environmental life sciences',\n",
        "    318730: 'Chemistry, except biochemistry',\n",
        "    338785: 'Physics and astronomy',\n",
        "    398895: 'Other physical and related sciences',\n",
        "    419295: 'Economics',\n",
        "    429295: 'Political and related sciences',\n",
        "    438995: 'Psychology',\n",
        "    449995: 'Sociology and anthropology',\n",
        "    459395: 'Other social sciences',\n",
        "    489395: 'Social and related sciences',\n",
        "    527250: 'Chemical engineering',\n",
        "    537260: 'Civil engineering',\n",
        "    547280: 'Electrical, electronics and communications engineering',\n",
        "    567350: 'Mechanical engineering',\n",
        "    587995: 'Other engineering',\n",
        "    611995: 'Health-related fields',\n",
        "    699995: 'Other science and engineering-related',\n",
        "    719995: 'Management and administration',\n",
        "    799995: 'Other non-science and engineering',\n",
        "    999999: 'Missing'\n",
        "}\n",
        "ndgmemg_mapping = {\n",
        "    1: 'Computer and mathematical sciences',\n",
        "    2: 'Biological, agricultural and environmental life sciences',\n",
        "    3: 'Physical and related sciences',\n",
        "    4: 'Social and related sciences',\n",
        "    5: 'Engineering',\n",
        "    6: 'Science and engineering-related fields',\n",
        "    7: 'Non-science and engineering fields',\n",
        "    99: 'Missing'\n",
        "}\n",
        "hdrgn_mapping = {1: 'New England', 2: 'Middle Atlantic', 3: 'East North Central', 4: 'West North Central', 5: 'South Atlantic', 6: 'East South Central', 7: 'West South Central', 8: 'Mountain', 9: 'Pacific', 60: 'Non-U.S.', 99: 'Missing'}\n",
        "lfstat_mapping = {1: 'employed full-time', 2: 'employed part-time', 3: 'unemployed', 4: 'not in labor force'}\n",
        "ocedrlp_mapping = {1: 'closely related', 2: 'somewhat related', 3: 'not related', 98: 'not applicable'}\n",
        "nocpr_mapping = {\n",
        "    182965: 'Postsecondary teachers-Computer and math sciences',\n",
        "    192895: 'Computer scientists and mathematicians',\n",
        "    222205: 'Biological and medical scientists',\n",
        "    282885: 'Postsecondary teachers-Life related sciences',\n",
        "    293995: 'Other life and related scientists',\n",
        "    311930: 'Chemistry, except biochemistry',\n",
        "    333305: 'Physicists and astronomers',\n",
        "    382995: 'Postsecondary teachers-Physical and related sciences',\n",
        "    393995: 'Other physical and related scientists',\n",
        "    412320: 'Economists',\n",
        "    432360: 'Psychologists',\n",
        "    482995: 'Postsecondary teachers-Social and related sciences',\n",
        "    483995: 'Other social scientists',\n",
        "    505005: 'Other engineers',\n",
        "    520850: 'Chemical engineers',\n",
        "    530860: 'Civil engineers',\n",
        "    540890: 'Electrical or computer hardware engineers',\n",
        "    560940: 'Mechanical engineers',\n",
        "    582800: 'Postsecondary teachers - engineering',\n",
        "    611995: 'Health-related occupations',\n",
        "    621995: 'science and engineering managers',\n",
        "    631995: 'Science and engineering pre-college teachers',\n",
        "    651995: 'Science and engineering pre-college teachers',\n",
        "    711410: 'Top and mid-level managers, executives, administrators',\n",
        "    711995: 'Other management related occupations',\n",
        "    735995: 'Non-science and engineering pre-college and post-secondary teachers',\n",
        "    799995: 'Other non-science and engineering occupations',\n",
        "    999998: 'Logical Skip'\n",
        "}\n",
        "emsec_mapping = {1: '2 year college or other school system', 2: '4 year college or medical institution', 3: 'Government', 4: 'Business or industry', 5: 'Non-US government', 98: 'Logical Skip'}\n",
        "govsup_mapping = {1: 'yes', 0: 'no', 98: 'not applicable'}\n",
        "nrrea_mapping = {\n",
        "    1: 'Pay, promotion opportunities',\n",
        "    2: 'Working conditions',\n",
        "    3: 'Job location',\n",
        "    4: 'Change in career or professional interests',\n",
        "    5: 'Family-related reasons',\n",
        "    6: 'Job in highest degree field not available',\n",
        "    7: 'Other reason for not working',\n",
        "    98: 'Logical Skip'\n",
        "}\n",
        "nrsec_mapping = {0: 'No second most important reason',\n",
        "1: 'Pay, promotion opportunities',\n",
        "2: 'Working conditions',\n",
        "3: 'Job location',\n",
        "4: 'Change in career or professional interests',\n",
        "5: 'Family-related reasons',\n",
        "6: 'Job in highest degree field not available',\n",
        "7: 'Other reason for not working',\n",
        "96: 'Blank',\n",
        "98: 'Not Applicable'}\n",
        "jobstatis_mapping = {1: 'Very satisfied', 2: 'Somewhat satisfied', 3: 'Somewhat dissatisfied', 4: 'Very dissatisfied', 98: 'Logical Skip'}\n",
        "nwlay_mapping = {1: 'Yes', 0: 'No', 98: 'Not Applicable'}\n",
        "nwnond_mapping = {1: 'Yes', 0: 'No', 98: 'Not applicable'}\n",
        "nwocna_mapping = {1: 'Yes', 0: 'No', 98: 'Not applicable'}\n",
        "nwstu_mapping = {1: 'Yes', 0: 'No', 98: 'Not Applicable'}\n",
        "\n",
        "gender_mapping = {1: 'male', 2: 'female'}\n",
        "\n",
        "\n",
        "# Create new columns with the mapped values\n",
        "jobedu_recoded['MINRTY_recoded'] = jobedu_recoded['MINRTY'].map(minrty_mapping)\n",
        "jobedu_recoded['RACETH_recoded'] = jobedu_recoded['RACETH'].map(raceth_mapping)\n",
        "jobedu_recoded['CHTOT_recoded'] = jobedu_recoded['CHTOT'].map(CHTOT_mapping)\n",
        "jobedu_recoded['NBAMED_recoded'] = jobedu_recoded['NBAMED'].map(nbamed_mapping)\n",
        "jobedu_recoded['DGRDG_recoded'] = jobedu_recoded['DGRDG'].map(dgrdg_mapping)\n",
        "jobedu_recoded['NDGMED_recoded'] = jobedu_recoded['NDGMED'].map(ndgmed_mapping)\n",
        "jobedu_recoded['NDGMEMG_recoded'] = jobedu_recoded['NDGMEMG'].map(ndgmemg_mapping)\n",
        "jobedu_recoded['HDRGN_recoded'] = jobedu_recoded['HDRGN'].map(hdrgn_mapping)\n",
        "jobedu_recoded['LFSTAT_recoded'] = jobedu_recoded['LFSTAT'].map(lfstat_mapping)\n",
        "jobedu_recoded['OCEDRLP_recoded'] = jobedu_recoded['OCEDRLP'].map(ocedrlp_mapping)\n",
        "jobedu_recoded['NOCPR_recoded'] = jobedu_recoded['NOCPR'].map(nocpr_mapping)\n",
        "jobedu_recoded['EMSEC_recoded'] = jobedu_recoded['EMSEC'].map(emsec_mapping)\n",
        "jobedu_recoded['GOVSUP_recoded'] = jobedu_recoded['GOVSUP'].map(govsup_mapping)\n",
        "jobedu_recoded['NRREA_recoded'] = jobedu_recoded['NRREA'].map(nrrea_mapping)\n",
        "jobedu_recoded['NRSEC_recoded'] = jobedu_recoded['NRSEC'].map(nrsec_mapping)\n",
        "jobedu_recoded['JOBSATIS_recoded'] = jobedu_recoded['JOBSATIS'].map(jobstatis_mapping)\n",
        "jobedu_recoded['NWLAY_recoded'] = jobedu_recoded['NWLAY'].map(nwlay_mapping)\n",
        "jobedu_recoded['NWNOND_recoded'] = jobedu_recoded['NWNOND'].map(nwnond_mapping)\n",
        "jobedu_recoded['NWOCNA_recoded'] = jobedu_recoded['NWOCNA'].map(nwocna_mapping)\n",
        "jobedu_recoded['NWSTU_recoded'] = jobedu_recoded['NWSTU'].map(nwstu_mapping)\n",
        "jobedu_recoded['GENDER_recoded'] = jobedu_recoded['GENDER'].map(gender_mapping)\n",
        "\n",
        "\n",
        "# Display the first few rows to verify the new columns\n",
        "display(jobedu_recoded.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRtFbRtWFA9l"
      },
      "source": [
        "Recode Num as Cat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv87UPx8FARo"
      },
      "outputs": [],
      "source": [
        "# Creating a new dataset by copying the original DataFrame\n",
        "jobedu_catnum = jobedu_recoded.copy()\n",
        "\n",
        "#TEST to see if the mapping works\n",
        "# Define a mapping for the 'GENDER' column\n",
        "gender_mapping = {'male':1, 'female':2}\n",
        "\n",
        "# Creating a new column 'GENDER_recoded' with the mapped values\n",
        "jobedu_catnum['GENDER_CAT'] = jobedu_catnum['GENDER'].map(gender_mapping)\n",
        "\n",
        "# Displaying the first few rows of the new dataset to see the new column\n",
        "display(jobedu_catnum.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmzh_3i4v0Rr"
      },
      "source": [
        "*Simone did the recode stacked bar chart*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c6bfc70"
      },
      "outputs": [],
      "source": [
        "# Select only the recoded columns\n",
        "recoded_columns_df = jobedu_recoded.filter(like='_recoded', axis=1).copy()\n",
        "\n",
        "# Display the first few rows of the new DataFrame\n",
        "display(recoded_columns_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJPrzf7PKZfT"
      },
      "outputs": [],
      "source": [
        "recoded_columns_df['SALARY']= jobedu_recoded['SALARY']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmMQvLxaKvSO"
      },
      "outputs": [],
      "source": [
        "display(recoded_columns_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Se6j19EK0bM"
      },
      "outputs": [],
      "source": [
        "jobvars = recoded_columns_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v453MeybdJsM"
      },
      "outputs": [],
      "source": [
        "jobedu_recoded.info()\n",
        "\n",
        "cat_cols_recoded = ['GENDER_recoded', 'MINRTY_recoded',\t'RACETH_recoded', 'NBAMED_recoded',\t'DGRDG_recoded',\t'NDGMED_recoded',\t'NDGMEMG_recoded',\t'HDRGN_recoded',\t'LFSTAT_recoded',\t'OCEDRLP_recoded', 'NOCPR_recoded',\t'EMSEC_recoded',\t'GOVSUP_recoded',\t'NRREA_recoded',\t'NRSEC_recoded',\t'JOBSATIS_recoded']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CiM8XNldBYR"
      },
      "outputs": [],
      "source": [
        "for col in cat_cols_recoded:\n",
        "    if col != 'YEAR': # Avoid grouping by Year with itself\n",
        "        try:\n",
        "            grouped_data_recoded = jobedu_recoded.groupby(['YEAR', col]).size().unstack()\n",
        "            if not grouped_data_recoded.empty:\n",
        "                grouped_data_recoded.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "                plt.title(f'Distribution of {col} by Year')\n",
        "                plt.xlabel('Year')\n",
        "                plt.ylabel('Count')\n",
        "                plt.legend(title=col, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"Could not create plot for {col}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6Gxv_c7_8w9"
      },
      "source": [
        "# Sankey Diagram\n",
        "*Simone made the Sankey diagrams*\n",
        "\n",
        "*Citation: https://www.youtube.com/watch?v=1aPAYcKV-S4*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqPFpzinvcWh"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02d2d914"
      },
      "outputs": [],
      "source": [
        "# Prepare data for Sankey diagram\n",
        "sankey_data = jobedu_recoded.groupby(['NBAMED_recoded', 'MINRTY_recoded']).size().reset_index(name='count')\n",
        "\n",
        "# Create lists for Sankey diagram\n",
        "source_labels = sankey_data['NBAMED_recoded'].unique().tolist()\n",
        "target_labels = sankey_data['MINRTY_recoded'].unique().tolist()\n",
        "\n",
        "# Create a combined list of all labels\n",
        "all_labels = source_labels + target_labels\n",
        "\n",
        "# Create a mapping from labels to indices\n",
        "label_indices = {label: i for i, label in enumerate(all_labels)}\n",
        "\n",
        "# Create source, target, and value lists for the Sankey diagram\n",
        "source_nodes = sankey_data['NBAMED_recoded'].map(label_indices).tolist()\n",
        "target_nodes = sankey_data['MINRTY_recoded'].map(label_indices).tolist()\n",
        "values = sankey_data['count'].tolist()\n",
        "\n",
        "# Create the Sankey diagram\n",
        "fig = go.Figure(data=[go.Sankey(\n",
        "    node=dict(\n",
        "        pad=15,\n",
        "        thickness=20,\n",
        "        line=dict(color=\"black\", width=0.5),\n",
        "        label=all_labels,\n",
        "        color=\"blue\"\n",
        "    ),\n",
        "    link=dict(\n",
        "        source=source_nodes,\n",
        "        target=target_nodes,\n",
        "        value=values\n",
        "    )\n",
        ")])\n",
        "\n",
        "fig.update_layout(title_text=\"Sankey Diagram of NBAMED_recoded and MINRTY_recoded\", font_size=10)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb474658"
      },
      "outputs": [],
      "source": [
        "# Prepare data for a three-variable Sankey diagram: GENDER_recoded (gender male, female) -> MINRTY_recoded (minority yes or no)-> DGRDG_recoded (degree recieved)\n",
        "\n",
        "# Group by all three columns to get counts for each combination\n",
        "sankey_data_3var = jobedu_recoded.groupby(['GENDER_recoded', 'MINRTY_recoded', 'DGRDG_recoded']).size().reset_index(name='count')\n",
        "\n",
        "# Create unique labels for all nodes\n",
        "all_labels_3var = list(sankey_data_3var['GENDER_recoded'].unique()) + \\\n",
        "                  list(sankey_data_3var['MINRTY_recoded'].unique()) + \\\n",
        "                  list(sankey_data_3var['DGRDG_recoded'].unique())\n",
        "\n",
        "# Create a mapping from labels to indices\n",
        "label_indices_3var = {label: i for i, label in enumerate(all_labels_3var)}\n",
        "\n",
        "# Create source, target, and value lists for the Sankey diagram\n",
        "\n",
        "# Links from GENDER_recoded to MINRTY_recoded\n",
        "source1 = sankey_data_3var['GENDER_recoded'].map(label_indices_3var).tolist()\n",
        "target1 = sankey_data_3var['MINRTY_recoded'].map(label_indices_3var).tolist()\n",
        "values1 = sankey_data_3var['count'].tolist()\n",
        "\n",
        "# Links from MINRTY_recoded to DGRDG_recoded\n",
        "# Need to aggregate counts for the links between MINRTY_recoded and DGRDG_recoded\n",
        "sankey_data_minrty_dgrdg = jobedu_recoded.groupby(['MINRTY_recoded', 'DGRDG_recoded']).size().reset_index(name='count')\n",
        "source2 = sankey_data_minrty_dgrdg['MINRTY_recoded'].map(label_indices_3var).tolist()\n",
        "target2 = sankey_data_minrty_dgrdg['DGRDG_recoded'].map(label_indices_3var).tolist()\n",
        "values2 = sankey_data_minrty_dgrdg['count'].tolist()\n",
        "\n",
        "\n",
        "# Combine the source, target, and value lists\n",
        "source_nodes_3var = source1 + source2\n",
        "target_nodes_3var = target1 + target2\n",
        "values_3var = values1 + values2\n",
        "\n",
        "\n",
        "# Create the Sankey diagram\n",
        "fig_3var = go.Figure(data=[go.Sankey(\n",
        "    node=dict(\n",
        "        pad=15,\n",
        "        thickness=20,\n",
        "        line=dict(color=\"black\", width=0.5),\n",
        "        label=all_labels_3var,\n",
        "        color=\"blue\"\n",
        "    ),\n",
        "    link=dict(\n",
        "        source=source_nodes_3var,\n",
        "        target=target_nodes_3var,\n",
        "        value=values_3var\n",
        "    )\n",
        ")])\n",
        "\n",
        "fig_3var.update_layout(title_text=\"Sankey Diagram of Gender, Minority Status, and Degree\", font_size=10)\n",
        "fig_3var.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6CbqQwYaGIG"
      },
      "outputs": [],
      "source": [
        "# Prepare data for a three-variable Sankey diagram: GENDER_recoded (gender male, female) -> MINRTY_recoded (minority yes or no)-> DGRDG_recoded (degree recieved)\n",
        "\n",
        "# Group by all three columns to get counts for each combination\n",
        "sankey_data_3var = jobedu_recoded.groupby(['GENDER_recoded', 'RACETH_recoded', 'DGRDG_recoded']).size().reset_index(name='count')\n",
        "\n",
        "# Create unique labels for all nodes\n",
        "all_labels_3var = list(sankey_data_3var['GENDER_recoded'].unique()) + \\\n",
        "                  list(sankey_data_3var['RACETH_recoded'].unique()) + \\\n",
        "                  list(sankey_data_3var['DGRDG_recoded'].unique())\n",
        "\n",
        "# Create a mapping from labels to indices\n",
        "label_indices_3var = {label: i for i, label in enumerate(all_labels_3var)}\n",
        "\n",
        "# Create source, target, and value lists for the Sankey diagram\n",
        "\n",
        "# Links from GENDER_recoded to MINRTY_recoded\n",
        "source1 = sankey_data_3var['GENDER_recoded'].map(label_indices_3var).tolist()\n",
        "target1 = sankey_data_3var['RACETH_recoded'].map(label_indices_3var).tolist()\n",
        "values1 = sankey_data_3var['count'].tolist()\n",
        "\n",
        "# Links from MINRTY_recoded to DGRDG_recoded\n",
        "# Need to aggregate counts for the links between MINRTY_recoded and DGRDG_recoded\n",
        "sankey_data_minrty_dgrdg = jobedu_recoded.groupby(['RACETH_recoded', 'DGRDG_recoded']).size().reset_index(name='count')\n",
        "source2 = sankey_data_minrty_dgrdg['RACETH_recoded'].map(label_indices_3var).tolist()\n",
        "target2 = sankey_data_minrty_dgrdg['DGRDG_recoded'].map(label_indices_3var).tolist()\n",
        "values2 = sankey_data_minrty_dgrdg['count'].tolist()\n",
        "\n",
        "\n",
        "# Combine the source, target, and value lists\n",
        "source_nodes_3var = source1 + source2\n",
        "target_nodes_3var = target1 + target2\n",
        "values_3var = values1 + values2\n",
        "\n",
        "\n",
        "# Create the Sankey diagram\n",
        "fig_3var = go.Figure(data=[go.Sankey(\n",
        "    node=dict(\n",
        "        pad=15,\n",
        "        thickness=20,\n",
        "        line=dict(color=\"black\", width=0.5),\n",
        "        label=all_labels_3var,\n",
        "        color=\"blue\"\n",
        "    ),\n",
        "    link=dict(\n",
        "        source=source_nodes_3var,\n",
        "        target=target_nodes_3var,\n",
        "        value=values_3var\n",
        "    )\n",
        ")])\n",
        "\n",
        "fig_3var.update_layout(title_text=\"Sankey Diagram of Gender, Racial Demographic, and Degree\", font_size=10)\n",
        "fig_3var.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaseIYVHXOWn"
      },
      "outputs": [],
      "source": [
        "label_indices = {label: i for i, label }\n",
        "\n",
        "source_indices ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RahsLapqvnhO"
      },
      "outputs": [],
      "source": [
        "deg.race_fig = go.Figure(data=[go.Sankey(\n",
        "    node = dict(\n",
        "      pad = 15,\n",
        "      thickness = 20,\n",
        "      line = dict(color = \"black\", width = 0.5),\n",
        "      label = [\"Bachelors\", \"Masters\", \"Doctorate\", \"Professional\"],\n",
        "      color = \"blue\"\n",
        "    ),\n",
        "    link = dict(\n",
        "      source = source\n",
        "      target = target\n",
        "      value = value\n",
        "  ))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVhREtDHHJhd"
      },
      "outputs": [],
      "source": [
        "# Define mappings for the additional columns\n",
        "minrty_mapping = {1: 'minority', 0: 'not minority'}\n",
        "raceth_mapping = {1: 'Asian', 2: 'White', 3: 'Under-represented Minority', 4: 'Other'}\n",
        "CHTOT_mapping = {0: 'No Children', 1: 'One Child', 2: 'One to Three Children', 3: 'Two or More Children', 4: 'More Than Three Children', 98: 'Not Applicable'}\n",
        "nbamed_mapping = {\n",
        "    198895: 'Computer and mathematical science',\n",
        "    226395: 'Biological sciences',\n",
        "    298895: 'Other biological, agricultural, environmental life sciences',\n",
        "    318730: 'Chemistry, except biochemistry',\n",
        "    338785: 'Physics and astronomy',\n",
        "    388795: 'Physical and related sciences',\n",
        "    398895: 'Other physical and related sciences',\n",
        "    419295: 'Economics',\n",
        "    429295: 'Political and related sciences',\n",
        "    438995: 'Psychology',\n",
        "    449995: 'Sociology and anthropology',\n",
        "    459395: 'Other social sciences',\n",
        "    489395: 'Social and related sciences',\n",
        "    527250: 'Chemical engineering',\n",
        "    537260: 'Civil engineering',\n",
        "    547280: 'Electrical, electronics and communications engineering',\n",
        "    567350: 'Mechanical engineering',\n",
        "    587995: 'Other engineering',\n",
        "    611995: 'Health-related fields',\n",
        "    699995: 'Other science and engineering-related',\n",
        "    719995: 'Management and administration',\n",
        "    799995: 'Other non-science and engineering',\n",
        "    999996: 'Blank',\n",
        "    999998: 'Logical skip',\n",
        "    999999: 'Missing'\n",
        "}\n",
        "\n",
        "dgrdg_mapping = {1: 'Bachelors', 2: 'Masters', 3: 'Doctorate', 4: 'Professional'}\n",
        "ndgmed_mapping = {1: 'yes', 2: 'no', 98: 'not applicable'}\n",
        "ndgmemg_mapping = {1: 'yes', 2: 'no', 98: 'not applicable'}\n",
        "hdrgn_mapping = {1: 'humanities', 2: 'social sciences', 3: 'natural sciences', 4: 'engineering', 5: 'health sciences', 6: 'education', 7: 'business', 8: 'other'}\n",
        "lfstat_mapping = {1: 'employed full-time', 2: 'employed part-time', 3: 'unemployed', 4: 'not in labor force'}\n",
        "ocedrlp_mapping = {1: 'closely related', 2: 'somewhat related', 3: 'not related', 98: 'not applicable'}\n",
        "nocpr_mapping = {\n",
        "    1: 'managerial and professional specialty occupations',\n",
        "    2: 'technical, sales, and administrative support occupations',\n",
        "    3: 'service occupations',\n",
        "    4: 'farming, forestry, and fishing occupations',\n",
        "    5: 'precision production, craft, and repair occupations',\n",
        "    6: 'operators, fabricators, and laborers',\n",
        "    98: 'not applicable'\n",
        "}\n",
        "emsec_mapping = {1: 'private for-profit', 2: 'private non-profit', 3: 'government', 4: 'self-employed', 5: 'family business', 98: 'not applicable'}\n",
        "govsup_mapping = {1: 'yes', 0: 'no', 98: 'not applicable'}\n",
        "nrrea_mapping = {\n",
        "    1: 'better pay',\n",
        "    2: 'better hours',\n",
        "    3: 'better working conditions',\n",
        "    4: 'personal/family reasons',\n",
        "    5: 'job in degree field not available',\n",
        "    6: 'preferred other work',\n",
        "    7: 'lack of skills/experience',\n",
        "    8: 'other',\n",
        "    98: 'not applicable'\n",
        "}\n",
        "nwlay_mapping = {1: 'yes', 0: 'no', 98: 'not applicable'}\n",
        "nwnond_mapping = {1: 'yes', 0: 'no', 98: 'not applicable'}\n",
        "nwocna_mapping = {1: 'yes', 0: 'no', 98: 'not applicable'}\n",
        "nwstu_mapping = {1: 'yes', 0: 'no', 98: 'not applicable'}\n",
        "\n",
        "\n",
        "# Create new columns with the mapped values\n",
        "jobedu_recoded['MINRTY_recoded'] = jobedu_recoded['MINRTY'].map(minrty_mapping)\n",
        "jobedu_recoded['RACETH_recoded'] = jobedu_recoded['RACETH'].map(raceth_mapping)\n",
        "jobedu_recoded['CHTOT_recoded'] = jobedu_recoded['CHTOT'].map(CHTOT_mapping)\n",
        "jobedu_recoded['NBAMED_recoded'] = jobedu_recoded['NBAMED'].map(nbamed_mapping)\n",
        "jobedu_recoded['DGRDG_recoded'] = jobedu_recoded['DGRDG'].map(dgrdg_mapping)\n",
        "jobedu_recoded['NDGMED_recoded'] = jobedu_recoded['NDGMED'].map(ndgmed_mapping)\n",
        "jobedu_recoded['NDGMEMG_recoded'] = jobedu_recoded['NDGMEMG'].map(ndgmemg_mapping)\n",
        "jobedu_recoded['HDRGN_recoded'] = jobedu_recoded['HDRGN'].map(hdrgn_mapping)\n",
        "jobedu_recoded['LFSTAT_recoded'] = jobedu_recoded['LFSTAT'].map(lfstat_mapping)\n",
        "jobedu_recoded['OCEDRLP_recoded'] = jobedu_recoded['OCEDRLP'].map(ocedrlp_mapping)\n",
        "jobedu_recoded['NOCPR_recoded'] = jobedu_recoded['NOCPR'].map(nocpr_mapping)\n",
        "jobedu_recoded['EMSEC_recoded'] = jobedu_recoded['EMSEC'].map(emsec_mapping)\n",
        "jobedu_recoded['GOVSUP_recoded'] = jobedu_recoded['GOVSUP'].map(govsup_mapping)\n",
        "jobedu_recoded['NRREA_recoded'] = jobedu_recoded['NRREA'].map(nrrea_mapping)\n",
        "jobedu_recoded['NWLAY_recoded'] = jobedu_recoded['NWLAY'].map(nwlay_mapping)\n",
        "jobedu_recoded['NWNOND_recoded'] = jobedu_recoded['NWNOND'].map(nwnond_mapping)\n",
        "jobedu_recoded['NWOCNA_recoded'] = jobedu_recoded['NWOCNA'].map(nwocna_mapping)\n",
        "jobedu_recoded['NWSTU_recoded'] = jobedu_recoded['NWSTU'].map(nwstu_mapping)\n",
        "\n",
        "\n",
        "# Display the first few rows to verify the new columns\n",
        "display(jobedu_recoded.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkzjmVBj8kcp"
      },
      "outputs": [],
      "source": [
        "display(recoded_columns_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeheBb6hJe4f"
      },
      "outputs": [],
      "source": [
        "jobvars['SALARY_log'] = np.log(jobvars['SALARY'])\n",
        "\n",
        "jobvars['intercept']=np.ones(jobvars.shape[0])\n",
        "\n",
        "def maxmin(z):\n",
        "    z = (z-np.min(z))/(np.max(z)-np.min(z))\n",
        "    return z\n",
        "\n",
        "jobvars.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSmlq8QsL6ZA"
      },
      "outputs": [],
      "source": [
        "jobvars['AGE']= jobedu_recoded['AGE']\n",
        "jobvars['CHTOT']= jobedu_recoded['CHTOT']\n",
        "jobvars['HRSWK']= jobedu_recoded['HRSWK']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZymdrfuSLWH"
      },
      "source": [
        "# RUNNING REGULARIZATION LASSO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6RZDFVILfBE"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Target variable:\n",
        "#y = df['price_log']\n",
        "y = jobvars['SALARY']\n",
        "\n",
        "# Ensure 'AGE' and 'HRSWK' are in jobvars before proceeding\n",
        "# These lines are moved from cell PSmlq8QsL6ZA to ensure variables are present\n",
        "jobvars['AGE']= jobedu_recoded['AGE']\n",
        "jobvars['HRSWK']= jobedu_recoded['HRSWK']\n",
        "\n",
        "# Numeric:\n",
        "var_num = ['AGE',\n",
        "       'HRSWK']\n",
        "X_num = jobvars.loc[:, var_num]\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "X_num_imputed = imputer.fit_transform(X_num)\n",
        "\n",
        "expander = PolynomialFeatures(degree=5, include_bias=False, interaction_only=True)\n",
        "Z = expander.fit_transform(X_num_imputed) # Pass the df into the expander to get powers/interactions of x and y\n",
        "names = expander.get_feature_names_out() # Get the names of these variables\n",
        "zdf = pd.DataFrame(data=Z, columns = names) # Create a new, expanded dataframe\n",
        "zdf = zdf.apply(maxmin)\n",
        "\n",
        "# Dummies:\n",
        "var_cat = [ 'MINRTY_recoded', 'RACETH_recoded','GENDER_recoded',\n",
        "       'CHTOT_recoded', 'NBAMED_recoded', 'DGRDG_recoded',\n",
        "        'HDRGN_recoded', 'LFSTAT_recoded']\n",
        "dummies = pd.DataFrame()\n",
        "for var in var_cat:\n",
        "    new_dummies = pd.get_dummies( jobvars.loc[:,var].copy(), dtype=int)\n",
        "    dummies = pd.concat([dummies, new_dummies], axis=1, ignore_index=True)\n",
        "\n",
        "# Create covariates:\n",
        "zdf = zdf.reset_index()\n",
        "dummies = dummies.reset_index()\n",
        "\n",
        "X = pd.concat([zdf, dummies], axis=1, ignore_index=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aUqV6RWRrwh"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lm = LinearRegression()\n",
        "lm= lm.fit(X_train,y_train)\n",
        "print('R^2, OLS, train: ', lm.score(X_train,y_train))\n",
        "print('R^2, OLS, test: ', lm.score(X_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF9jTHuU9zln"
      },
      "outputs": [],
      "source": [
        "# Create a working copy to avoid modifying original jobvars unexpectedly\n",
        "jobvars_working = jobvars.copy()\n",
        "\n",
        "# Replace sentinel values (9999998, 9999999) with NaN in SALARY\n",
        "# Based on typical IPUMS codes and observation of jobedu_recoded.head()\n",
        "jobvars_working['SALARY'] = jobvars_working['SALARY'].replace({9999998.0: np.nan, 9999999.0: np.nan})\n",
        "\n",
        "# Drop rows where SALARY is NaN\n",
        "jobvars_working.dropna(subset=['SALARY'], inplace=True)\n",
        "\n",
        "# Update y and X based on the cleaned jobvars_working\n",
        "y = jobvars_working['SALARY']\n",
        "\n",
        "# Numeric features:\n",
        "var_num = ['AGE', 'HRSWK']\n",
        "X_num = jobvars_working.loc[:, var_num]\n",
        "\n",
        "# Impute NaNs in numeric features\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "# Fit and transform, maintaining DataFrame structure for index alignment\n",
        "X_num_imputed_array = imputer.fit_transform(X_num)\n",
        "zdf = pd.DataFrame(data=X_num_imputed_array, columns=X_num.columns, index=jobvars_working.index)\n",
        "\n",
        "# Apply polynomial features and scaling\n",
        "expander = PolynomialFeatures(degree=5, include_bias=False, interaction_only=True)\n",
        "Z = expander.fit_transform(zdf)\n",
        "names = expander.get_feature_names_out(zdf.columns) # Use original column names for better feature names\n",
        "zdf_expanded = pd.DataFrame(data=Z, columns = names, index=jobvars_working.index)\n",
        "zdf_expanded = zdf_expanded.apply(maxmin)\n",
        "\n",
        "# Categorical features (dummies):\n",
        "var_cat = [ 'MINRTY_recoded', 'RACETH_recoded','GENDER_recoded',\n",
        "       'CHTOT_recoded', 'NBAMED_recoded', 'DGRDG_recoded',\n",
        "        'HDRGN_recoded', 'LFSTAT_recoded']\n",
        "\n",
        "dummies_list = []\n",
        "for var in var_cat:\n",
        "    # Ensure all categorical variables are treated as strings before get_dummies\n",
        "    new_dummies = pd.get_dummies(jobvars_working.loc[:,var].astype(str), prefix=var, dtype=int)\n",
        "    dummies_list.append(new_dummies)\n",
        "\n",
        "dummies = pd.concat(dummies_list, axis=1)\n",
        "\n",
        "# Create covariates by concatenating expanded numeric features and dummies\n",
        "# Ensure indices align correctly.\n",
        "X = pd.concat([zdf_expanded, dummies], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYVZIDNxR6Va"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.histplot(lm.coef_, bins = 50, stat='proportion')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fp2eq_yR_Y9"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Run LASSO:\n",
        "reg = Lasso(alpha=2.5,\n",
        "            warm_start=True,\n",
        "            max_iter=2500)\n",
        "reg.fit(X_train,y_train)\n",
        "\n",
        "print('R^2, Lasso, train: ', reg.score(X_train,y_train))\n",
        "print('R^2, Lasso, test: ', reg.score(X_test,y_test))\n",
        "\n",
        "sdf = pd.DataFrame({'variable': X.columns, 'slope':reg.coef_})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8q6iSB-UPRk"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import lasso_path\n",
        "\n",
        "eps = 5e-10\n",
        "\n",
        "print(\"Computing regularization path using the lasso...\")\n",
        "alphas_lasso, coefs_lasso, _ = lasso_path(X_train, y_train, eps=eps)\n",
        "\n",
        "for coef_lasso in coefs_lasso:\n",
        "    l1 = plt.semilogx(alphas_lasso, coef_lasso)\n",
        "\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"coefficients\")\n",
        "plt.title(\"Lasso Paths\")\n",
        "plt.axis(\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX0dSc-AUVK8"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.histplot(sdf['slope'], bins = 50, stat='proportion')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Sq1DIOWtBu"
      },
      "source": [
        "It sets most of the coefficients to zero"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ASSO Regression Analysis\n",
        "The LASSO regression model was applied to the dataset to evaluate the predictive strength of demographic and educational features while performing variable selection. Using an alpha of 2.5, LASSO effectively shrunk many coefficients to zero, allowing us to identify the most influential predictors of salary.\n",
        "\n",
        "Key Observations:\n",
        "Only a subset of the features retained non-zero coefficients, highlighting which factors have the strongest association with salary.\n",
        "\n",
        "Continuous variables such as age and hours worked (after polynomial expansion and scaling) were significant, but many higher-degree interaction terms were shrunk to zero.\n",
        "\n",
        "Among categorical variables, certain race and gender indicators, degree types, and major fields emerged as important predictors.\n",
        "\n",
        "This model provides a clear picture of which features are consistently impactful while controlling for multicollinearity and overfitting.\n",
        "\n",
        "\n",
        "Strengths & Limitations:\n",
        "\n",
        "Strengths:\n",
        "\n",
        "Feature selection helps focus on meaningful predictors; interpretable coefficients.\n",
        "\n",
        "\n",
        "Limitations:\n",
        "\n",
        "LASSO assumes linear relationships and may not capture complex interactions that exist in the data.\n"
      ],
      "metadata": {
        "id": "Pm9Ap7_FjEt8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBfYA2n6WxIx"
      },
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0sLeNZs5cFF"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier # Import the tree classifier\n",
        "from sklearn.tree import plot_tree # Plot the tree\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "import matplotlib.pyplot as plt # Ensure matplotlib is imported for plotting\n",
        "\n",
        "# Create a working copy to avoid modifying original jobvars unexpectedly\n",
        "jobvars_dt = jobvars.copy()\n",
        "\n",
        "# Ensure 'AGE' and 'HRSWK' are present in jobvars_dt by adding them from jobedu_recoded\n",
        "jobvars_dt['AGE'] = jobedu_recoded['AGE']\n",
        "jobvars_dt['HRSWK'] = jobedu_recoded['HRSWK']\n",
        "\n",
        "# Ensure 'AGE' and 'HRSWK' are numeric. Coerce non-numeric to NaN if necessary.\n",
        "jobvars_dt['AGE'] = pd.to_numeric(jobvars_dt['AGE'], errors='coerce')\n",
        "jobvars_dt['HRSWK'] = pd.to_numeric(jobvars_dt['HRSWK'], errors='coerce')\n",
        "\n",
        "\n",
        "# Replace sentinel values (9999998, 9999999) with NaN in SALARY\n",
        "jobvars_dt['SALARY'] = jobvars_dt['SALARY'].replace({9999998.0: np.nan, 9999999.0: np.nan})\n",
        "\n",
        "# Drop rows where SALARY is NaN, as SALARY is the target variable\n",
        "jobvars_dt.dropna(subset=['SALARY'], inplace=True)\n",
        "\n",
        "# Target variable:\n",
        "y_dt = jobvars_dt['SALARY']\n",
        "\n",
        "# Drop columns that are either target or not intended as features for the tree\n",
        "X_dt = jobvars_dt.drop(['SALARY', 'SALARY_log', 'intercept', 'CHTOT'], axis=1, errors='ignore')\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numeric_cols = X_dt.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = X_dt.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "## Split the Sample into Training and Testing Sets:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=104)\n",
        "\n",
        "# Impute missing values in numerical features\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "# Only attempt imputation if there are actually numeric columns to impute\n",
        "if numeric_cols:\n",
        "    X_dt[numeric_cols] = imputer.fit_transform(X_dt[numeric_cols])\n",
        "else:\n",
        "    # If no numerical columns, then this part of the data preparation is skipped.\n",
        "    print(\"No numerical columns found for imputation. Proceeding with categorical features only.\")\n",
        "\n",
        "# One-hot encode categorical features\n",
        "X_dt_processed = pd.get_dummies(X_dt, columns=categorical_cols, dtype=int)\n",
        "\n",
        "# Fit decision tree:\n",
        "cart = DecisionTreeClassifier(random_state=42) # Create a classifier object with a random state for reproducibility\n",
        "cart = cart.fit(X_dt_processed, y_dt) # Fit the classifier\n",
        "\n",
        "# Visualize results:\n",
        "plt.figure(figsize=(20,15))\n",
        "# Limiting the depth of the tree for visualization purposes to avoid overly large plots\n",
        "plot_tree(cart, filled=True, feature_names=X_dt_processed.columns.tolist(), max_depth=3, fontsize=10)\n",
        "plt.title(\"Decision Tree Visualization (Max Depth 3)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decision tree classifier was employed to explore nonlinear interactions between features and to identify major splits influencing salary. Categorical variables were one-hot encoded, and missing numeric values were median-imputed.\n",
        "Key Observations:\n",
        "\n",
        "Major splits in the tree often involved demographic variables like gender and race, as well as educational factors such as degree type and major.\n",
        "Interactions were clearly captured, e.g., certain majors had different salary patterns depending on gender or age.\n",
        "\n",
        "The max-depth-limited tree (depth=3) allowed for interpretable visualization of primary drivers of salary without overcomplicating the model.\n",
        "Strengths & Limitations:\n",
        "Strengths: Easily interpretable; captures nonlinear relationships and interactions.\n",
        "Limitations: Single trees may overfit; accuracy is generally lower than ensemble methods.\n"
      ],
      "metadata": {
        "id": "nIw58AVTj5dK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4AVhQIBW6UJ"
      },
      "source": [
        "# Random Forest Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "22531337",
        "outputId": "ef03d9f1-14bb-4811-9e6b-4181b5a8ea91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (949948, 31)\n",
            "      SALARY   AGE  HRSWK\n",
            "0    31000.0  33.0    NaN\n",
            "1  9999998.0  38.0    NaN\n",
            "2    48000.0  48.0    NaN\n",
            "3  9999999.0  48.0    NaN\n",
            "4    30000.0  28.0    NaN\n",
            "Final feature matrix shape: (802936, 43)\n",
            "Train size: (642348, 43) Test size: (160588, 43)\n",
            "Training Random Forest Regressor...\n",
            "Training complete.\n",
            "\n",
            "Random Forest Regression Model Performance:\n",
            "R-squared: 0.3291\n",
            "Mean Squared Error (MSE): 935085806.87\n",
            "Mean Absolute Error (MAE): 22625.00\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# Random Forest Regression on SALARY\n",
        "# ================================\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# 1. Load data\n",
        "# Make sure the CSV is in /content or update the path if needed\n",
        "jobedu = pd.read_csv('/content/highered_00001.csv')\n",
        "\n",
        "# Quick check\n",
        "print(\"Data shape:\", jobedu.shape)\n",
        "print(jobedu[['SALARY', 'AGE', 'HRSWK']].head())\n",
        "\n",
        "# 2. Clean target variable (SALARY)\n",
        "# IPUMS-style sentinel values treated as missing\n",
        "jobedu['SALARY'] = jobedu['SALARY'].replace({9999998.0: np.nan, 9999999.0: np.nan})\n",
        "\n",
        "# Drop rows where SALARY is missing (we can't train a regressor on missing targets)\n",
        "jobedu = jobedu.dropna(subset=['SALARY'])\n",
        "\n",
        "# 3. Select features for the model\n",
        "# Numeric features\n",
        "numeric_features = ['AGE', 'HRSWK']\n",
        "\n",
        "# Categorical features (coded as integers in the original data)\n",
        "# You can adjust this list depending on what you want to include\n",
        "categorical_features = [\n",
        "    'GENDER',      # 1, 2\n",
        "    'MINRTY',      # 0, 1\n",
        "    'RACETH',      # 1–4\n",
        "    'NBAMED',      # bachelor's major code\n",
        "    'DGRDG',       # degree type\n",
        "    'NDGMEMG',     # highest degree major group\n",
        "    'EMSEC',       # employer sector\n",
        "    'LFSTAT'       # labor force status\n",
        "]\n",
        "\n",
        "# Keep only rows where these columns exist (and keep their current values)\n",
        "feature_cols = numeric_features + categorical_features\n",
        "jobvars = jobedu[feature_cols + ['SALARY']].copy()\n",
        "\n",
        "# 4. Separate X (features) and y (target)\n",
        "y = jobvars['SALARY']\n",
        "X = jobvars.drop(columns=['SALARY'])\n",
        "\n",
        "# 5. Handle numeric missing values with median imputation\n",
        "imputer_num = SimpleImputer(strategy='median')\n",
        "X_num = pd.DataFrame(\n",
        "    imputer_num.fit_transform(X[numeric_features]),\n",
        "    columns=numeric_features,\n",
        "    index=X.index\n",
        ")\n",
        "\n",
        "# 6. One-hot encode categorical variables\n",
        "# Treat them as categorical and expand to dummies\n",
        "X_cat = pd.get_dummies(X[categorical_features].astype('category'),\n",
        "                       columns=categorical_features,\n",
        "                       drop_first=True,  # avoid full multicollinearity\n",
        "                       dtype=int)\n",
        "\n",
        "# 7. Combine numeric + categorical features into final design matrix\n",
        "X_processed = pd.concat([X_num, X_cat], axis=1)\n",
        "\n",
        "print(\"Final feature matrix shape:\", X_processed.shape)\n",
        "\n",
        "# 8. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_processed, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)\n",
        "\n",
        "# 9. Initialize and train Random Forest Regressor\n",
        "# If this is slow, you can reduce n_estimators or set max_depth.\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,   # try 50 if Colab is slow\n",
        "    random_state=42,\n",
        "    n_jobs=-1           # use all available cores\n",
        ")\n",
        "\n",
        "print(\"Training Random Forest Regressor...\")\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# 10. Make predictions and evaluate\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(\"\\nRandom Forest Regression Model Performance:\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjl0wPtVW-ob"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZy69jRTx5TR"
      },
      "source": [
        "Continued draft of Methods & Results:\n",
        "\n",
        "In examining the disparities of the labor market, especially amongst recent U.S. college graduates, our team has gone on to apply a combination of descriptive statistics, regression modeling, and data visualization techniques. Our approach is centered on identifying how race, gender, and field of study interact with employment status, income, and job satisfaction. We used … for data wrangling and exploratory analysis and … for statistical modeling and inference.\n",
        "\n",
        "After merging survey data from NSCG, SDR, NSRCG, and ISDR, we performed several preprocessing steps to ensure that we keep the data consistent across time and manipulation. Beginning with the standardization of categorical variables such as race, gender, and major into unified labels throughout the years. Filtering our incomplete responses that lacked key demographic or employment information…\n",
        "\n",
        "Insert figure 1\n",
        "\n",
        "We continued with exploratory data analysis to identify general patterns and potential outliers. Then, to assess the impact of demographic and educational factors on job outcomes, we employed several modeling techniques. We used descriptive statistics and correlation analysis where we summarized the means and distributions for incomes, job satisfaction, and employment type across demographic groups. We used linear regression models to estimate how income varies with predictors such as gender, race, and major while controlling for degree level and technical skills….\n",
        "\n",
        "Insert visualizations\n",
        "\n",
        "We randomly split the dataset into training (80%) and testing (20%) sets to evaluate model performance. Continuous variables were standardized, and categorical variables were encoded. Model performance was assessed through … R^2 for regression accuracy, confusion matrices and ROC curves for classification performance, and cross-validation to test generalizability across the surveyed years.\n",
        "\n",
        "Insert visualization\n",
        "\n",
        "Our initial analysis revealed several key findings…\n",
        "\n",
        "Insert visualization\n",
        "\n",
        "While our models explain a significant portion of the variance in labor market outcomes (R^2=…), unobservable factors such as internship experiences, networking opportunities, and regional job availability may influence these results. Additionally, due to logical skips and missing survey responses, some demographic groups may be underrepresented, potentially biasing estimates. Nonetheless, our preliminary findings align with broader labor market trends and highlight the continuing need to address inequities across demographic lines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKNMYKjY_Ewv"
      },
      "source": [
        "Conclusion: One to two pages summarizing the project, defending it from criticism, and suggesting additional work that was outside the scope of the project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnzKC6JF_VWq"
      },
      "source": [
        "References: Cite the data sources and relevant works you have referenced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AowoL4SX_V7_"
      },
      "source": [
        "Minnesota Population Center. IPUMS Higher Ed: Version 1.0 [dataset]. Minneapolis, MN: University of Minnesota, 2016. https://doi.org/10.18128/D100.V1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO8v9EzM_ZY8"
      },
      "source": [
        " National Survey of College Graduates 1993, SESTAT\n",
        "      Survey of Doctorate Recipients 1993, SESTAT\n",
        "      National Survey of Recent College Graduates 1993, SESTAT\n",
        "      National Survey of College Graduates 1995, SESTAT\n",
        "      Survey of Doctorate Recipients 1995, SESTAT\n",
        "      National Survey of Recent College Graduates 1995, SESTAT\n",
        "      National Survey of College Graduates 1997, SESTAT\n",
        "      Survey of Doctorate Recipients 1997, SESTAT\n",
        "      National Survey of Recent College Graduates 1997, SESTAT\n",
        "      National Survey of College Graduates 1999, SESTAT\n",
        "      Survey of Doctorate Recipients 1999, SESTAT\n",
        "      National Survey of Recent College Graduates 1999, SESTAT\n",
        "      National Survey of College Graduates 2003, SESTAT\n",
        "      Survey of Doctorate Recipients 2003, SESTAT\n",
        "      National Survey of Recent College Graduates 2003, SESTAT\n",
        "      National Survey of College Graduates 2006, SESTAT\n",
        "      Survey of Doctorate Recipients 2006, SESTAT\n",
        "      National Survey of Recent College Graduates 2006, SESTAT\n",
        "      National Survey of College Graduates 2008, SESTAT\n",
        "      Survey of Doctorate Recipients 2008, SESTAT\n",
        "      National Survey of Recent College Graduates 2008, SESTAT\n",
        "      National Survey of College Graduates 2010, SESTAT\n",
        "      Survey of Doctorate Recipients 2010, SESTAT\n",
        "      National Survey of Recent College Graduates 2010, SESTAT\n",
        "      National Survey of College Graduates 2013, SESTAT\n",
        "      Survey of Doctorate Recipients 2013, SESTAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAR5xlEy_edy"
      },
      "source": [
        "Appendix: If you have a significant number of additional plots or tables that you feel are essential to the project, you can put any amount of extra content (after the\n",
        "References section) at the end and mention it in the body of the paper. Be aware that\n",
        "readers are not required to read the appendix, and that the main paper content\n",
        "should stand on its own."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}